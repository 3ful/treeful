[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Treeful Docs",
    "section": "",
    "text": "About this Book\nThis book is the documentation for Treeful, an R Shiny application helping people plant trees based on species distribution modelling."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "Treeful Docs",
    "section": "Contribute",
    "text": "Contribute"
  },
  {
    "objectID": "index.html#who-is-behind-this",
    "href": "index.html#who-is-behind-this",
    "title": "Treeful Docs",
    "section": "Who is behind this",
    "text": "Who is behind this\n\nJakob Kutsch\nChristoph Trost"
  },
  {
    "objectID": "index.html#about-this-project-and-technical-approach",
    "href": "index.html#about-this-project-and-technical-approach",
    "title": "Treeful Docs",
    "section": "About this Project and technical Approach",
    "text": "About this Project and technical Approach\nThis project was funded from the German ministry for Education and Research from March 1st 2023 to September 1st 2023. The purpose is to build a shiny app that allows user to explore habitat shapes of trees in Europe. These habitat shapes are plots of existing trees from a merged database of close to 9 million trees. We extracted climatic variables present at each tree location. These shapes allow users to compare with their own location, a potential planting site, for the past and future."
  },
  {
    "objectID": "index.html#infrastructure",
    "href": "index.html#infrastructure",
    "title": "Treeful Docs",
    "section": "Infrastructure",
    "text": "Infrastructure\nThe docker-compose file\n\nETL Container\nThis container runs through the main script, fetching tree location data, getting bioclimatic variables, extracting those from the tree locations and writing everything to Postgis.\n\n\nPostgis Database Container\nAll processed data from the ETL pipeline ends up in here. We chose to also write rasters to postgis, although rasters for projects like this are typically stored on disk. Our final shiny app will never read an entire raster but merely extract values from user locations.\n\n\nShiny Container\nThe actual app runs as docker swarm and connects to the Postgis database."
  },
  {
    "objectID": "index.html#treeful-etl-pipeline-obtaining-and-transforming-tree-occurrence-data",
    "href": "index.html#treeful-etl-pipeline-obtaining-and-transforming-tree-occurrence-data",
    "title": "Treeful Docs",
    "section": "Treeful ETL Pipeline: Obtaining and transforming tree occurrence data",
    "text": "Treeful ETL Pipeline: Obtaining and transforming tree occurrence data\nThis section documents how tree occurrence data is obtained. At the end, we will have our tree locations with the corresponding bioclimatic variables in a postgis database. Additionally we write rasters and a few master data tables into postgis.\nFirst, we obtain data not included in this repository, namely EU Forest and Copernicus CDS raster files. These downloads will take a while. (Some datasets are neither downloaded, nor included in this repo. Theyâ€™ve been shared privately with us)\n\n#################### Trees4F from figshare ###################\nif (!file.exists(\"2_Data/0_raw_data/EUforestspecies_AMauri.csv\")) {\n  download.file(\"https://springernature.figshare.com/ndownloader/files/6662535\", destfile = \"2_Data/0_raw_data/EUforestspecies_AMauri.csv\")\n}\n\n#################### COPERNICUS CDS Download ###################\n# REGISTER YOURSELF AND ENTER DETAILS FROM HERE:\n# https://cds.climate.copernicus.eu/user/register\n# Beware the user is the UID at the bottom of your user page.\n\nuid <- Sys.getenv(\"COPERNICUS_UID\")\n\nwf_set_key(user=uid, # \n           key=Sys.getenv(\"COPERNICUS_KEY\"), \n           service=\"cds\")\n\nrequest <- list(\n  region = \"europe\",\n  variable = c(\"annual_mean_temperature\", \"annual_precipitation\", \"isothermality\", \"maximum_temperature_of_warmest_month\", \"mean_diurnal_range\", \"mean_temperature_of_coldest_quarter\", \"mean_temperature_of_driest_quarter\", \"mean_temperature_of_warmest_quarter\", \"mean_temperature_of_wettest_quarter\", \"minimum_temperature_of_coldest_month\", \"precipitation_of_coldest_quarter\", \"precipitation_of_driest_month\", \"precipitation_of_driest_quarter\", \"precipitation_of_warmest_quarter\", \"precipitation_of_wettest_month\", \n\"precipitation_of_wettest_quarter\", \"precipitation_seasonality\", \"temperature_annual_range\", \"temperature_seasonality\", \"volumetric_soil_water\"),\n  derived_variable = \"annual_mean\",\n  model = \"hadgem2_cc\",\n  ensemble_member = \"r1i1p1\",\n  experiment = \"rcp4_5\",\n  statistic = \"mean\",\n  version = \"1.0\",\n  format = \"zip\",\n  dataset_short_name = \"sis-biodiversity-cmip5-regional\",\n  target = \"download.zip\"\n)\n\nncfile <- wf_request(\n  user = uid,\n  request = request,   \n  transfer = TRUE,  \n  path = \"2_Data/0_raw_data/future\",\n  verbose = FALSE\n)\n\n\nrequest <- list(\n  region = \"europe\",\n  origin = \"era5\",\n  variable = c(\"annual_mean_temperature\", \"annual_precipitation\", \"isothermality\", \"maximum_temperature_of_warmest_month\", \"mean_diurnal_range\", \"mean_temperature_of_coldest_quarter\", \"mean_temperature_of_driest_quarter\", \"mean_temperature_of_warmest_quarter\", \"mean_temperature_of_wettest_quarter\", \"minimum_temperature_of_coldest_month\", \"precipitation_of_coldest_quarter\", \"precipitation_of_driest_month\", \"precipitation_of_driest_quarter\", \"precipitation_of_warmest_quarter\", \"precipitation_of_wettest_month\", \n               \"precipitation_of_wettest_quarter\", \"precipitation_seasonality\", \"temperature_annual_range\", \"temperature_seasonality\", \"volumetric_soil_water\"),\n  derived_variable = \"annual_mean\",\n  statistic = \"mean\",\n  version = \"1.0\",\n  format = \"zip\",\n  dataset_short_name = \"sis-biodiversity-era5-regional\",\n  target = \"download.zip\"\n)\n\nncfile <- wf_request(\n  user = uid,\n  request = request,   \n  transfer = TRUE,  \n  path = \"2_Data/0_raw_data/past\",\n  verbose = FALSE\n)\n\n\nif (file.exists(\"2_Data/0_raw_data/past/download.zip\")) {\n  utils::unzip(\"2_Data/0_raw_data/past/download.zip\", exdir = \"2_Data/0_raw_data/past/\")\n  file.remove(\"2_Data/0_raw_data/past/download.zip\")\n}\nif (file.exists(\"2_Data/0_raw_data/future/download.zip\")) {\n  utils::unzip(\"2_Data/0_raw_data/future/download.zip\", exdir = \"2_Data/0_raw_data/future/\")\n  file.remove(\"2_Data/0_raw_data/future/download.zip\")\n}\n\n\nGetting Tree Location Data\nTo produce reliable climate envelopes we would like to obtain as many trees from various biogeographical regions of Europe. This section details how we process them and produces a database with these occurrences:\n\n\n\nTree occurences in treeful per source database\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe are not doing any sampling on tree occurrences. This means that small areas with many tree records will produce a higher density in the climate envelopes. In the current state of our app and plots, this does reflect to the user.\n\n\n\nAcademic data sources\nWe used the TRY plant trait database and EU Forest. Both contain tree locations and botanical names. Both are academic, high-quality datasets with extensive data cleaning in place. We used EU Forest to generate our master list of botanical names. These trees will subsequently be used. A simple fuzzy matching is used to ensure Sorbus Torminalis and Sorbus-torminalis are matched as Sorbus torminalis (We used a conservative string dist=1).\n\n\nCode\n############ Load Packages ############ \nif(!require(librarian)) install.packages(\"librarian\")\nlibrary(librarian)\nshelf(data.table,stringr,stringdist,fuzzyjoin,dplyr)\n\n############ Define Function for fuzzy matching ############ \nflatten_names <- function(tree_name) {\n  tree_name <-  str_replace_all(tree_name,\" \", \"\") %>%\n    str_replace_all(.,\"\\\\.\",\"\") %>%\n    str_replace_all(.,\"\\\\-\",\"\") %>%\n    str_replace_all(.,\"\\\\'\",\"\") %>%\n    tolower(.)\n}\n\n\nget_fuzzy_plant_names <- function(plant_database, master_list, max_string_dist=3){\n# Prepare for matching\n# Extract unique names\ndatabase_names <- plant_database[,.(latin_name=unique(latin_name))]\n\n# Flatten names\nmaster_list[,flat_name:=flatten_names(latin_name)]\ndatabase_names[,flat_name:=flatten_names(latin_name)]\n\n# Match\nstringdist <- as.data.table(\n                stringdist_join(database_names, master_list, \n                by = c(\"flat_name\" = \"flat_name\"), \n                mode = \"inner\", \n                method = \"dl\", \n                max_dist = max_string_dist, \n                distance_col = \"dist\"))\n# Clean matching\nstringdist <- stringdist[,.(data_set_name=latin_name.x,master_list_name=latin_name.y,distance=dist)]\nstringdist <- setorder(stringdist,-distance)\nreturn(stringdist)\n# As an example of possible other matching functions:\n# amatch <-  amatch(database_names[,flat_name], master_list[,flat_name], maxDist = 0.1)\n}\n############ MAIN: Create master list of botanical names ############ \n# Load datasets\n# Use trees4EU as master list, more coverage\nmaster_list <- data.table::fread(\"2_Data/0_raw_data/EUforestspecies_AMauri.csv\") %>% \n  janitor::clean_names() %>% \n  dplyr::select(latin_name = species_name) %>% \n  distinct()\nmaster_list <- na.omit(master_list)\n\n############ OPEN TREES DATA BASE ############ \n# Load and prepare database\n\n# Load database\nopen_trees_db <- fread(\"2_Data/1_output/all_merged.csv\")\nsetnames(open_trees_db,\"species\",\"latin_name\")\nopen_trees_db <- na.omit(open_trees_db)\n\n# Match db names\nmax_string_dist = 1 \nopen_trees_matching_table<- get_fuzzy_plant_names(open_trees_db, master_list, max_string_dist)\nopen_trees_matching_table_selection <- open_trees_matching_table[distance<=1,]\n# Filter database\nopen_trees_db_selection <- open_trees_db[open_trees_matching_table_selection[,.(data_set_name,master_list_name)], on = c(latin_name = \"data_set_name\"), nomatch = NULL]\n# open_trees_db_selection2 <- open_trees_db[latin_name %in% open_trees_matching_table_selection[,data_set_name]]\n\n############ TRY DATA BASE ############ \n# Load database\nif (file.exists(\"2_Data/0_raw_data/tree_georef_3.txt\")) {\n  try_trees <- data.table::fread(\"2_Data/0_raw_data/tree_georef_3.txt\") %>% \n    janitor::clean_names()\n  \n  setnames(try_trees,\"acc_species_name\",\"latin_name\")\n  try_trees <- na.omit(try_trees)\n  \n  \n  # Match db names\n  max_string_dist = 1 \n  try_trees_matching_table<- get_fuzzy_plant_names(try_trees, master_list, max_string_dist)\n  try_trees_matching_table_selection <- try_trees_matching_table[distance<=1,]\n  # Filter database\n  try_trees_selection <- try_trees[try_trees_matching_table_selection[,.(data_set_name,master_list_name)], on = c(latin_name = \"data_set_name\"), nomatch = NULL]\n  # open_trees_db_selection2 <- open_trees_db[latin_name %in% open_trees_matching_table_selection[,data_set_name]]\n  \n  \n} else {\n  try_trees_selection <- tibble(master_list_name = character(), \n                                tree_georef_1_std_value = numeric(),\n                                obs_data_std_value = numeric())\n}\n\n\n############ Trees4F DATA BASE ############ \n# Load and prepare database\n\n# Load database\ntrees4f_db <- data.table::fread(\"2_Data/0_raw_data/EUforestspecies_AMauri.csv\") %>% \n  janitor::clean_names()\n\nsetnames(trees4f_db,\"species_name\",\"latin_name\")\ntrees4f_db <- na.omit(trees4f_db)\n\n# Match db names\nmax_string_dist = 3 \ntrees4f_db_matching_table<- get_fuzzy_plant_names(trees4f_db, master_list, max_string_dist)\ntrees4f_db_matching_table_selection <- trees4f_db_matching_table[distance<=1,]\n# Filter database\ntrees4f_db_selection <- trees4f_db[trees4f_db_matching_table_selection[,.(data_set_name,master_list_name)], on = c(latin_name = \"data_set_name\"), nomatch = NULL]\n# open_trees_db_selection2 <- open_trees_db[latin_name %in% open_trees_matching_table_selection[,data_set_name]]\n\n\n\n\n# enhance master list with GBIF taxo IDs and write master list to file\ntree_master_list <- master_list %>% \n  mutate(gbif_taxo_id = name_backbone_checklist(name=.$latin_name)$usageKey) %>% \n  # remove  unmatched or genus level taxo matches\n  filter(str_length(gbif_taxo_id) > 5 & !is.na(gbif_taxo_id))\n\nfwrite(tree_master_list,\"2_Data/1_output/eu_native_trees_master.csv\")\ntree_master_list <- fread(\"2_Data/1_output/eu_native_trees_master.csv\") \n\n\nrm(open_trees_db, open_trees_matching_table, open_trees_matching_table_selection, try_trees, try_trees_matching_table, \n   try_trees_matching_table_selection, trees4f_db, trees4f_db_matching_table, trees4f_db_matching_table_selection\n   )\n\n#EOF\n\n\nWe finish this file with rgbif::name_backbone_checklist() where we obtain the GBIF taxo ID for each botanical name. This is part of our master data list.\n\n\nEuropean Tree Cadastres\nWe wanted to include the proliferating corpus of tree cadastres of European cities. We collected two dozen or so data sources into this file. Since theyâ€™re all unharmonized, extensive data cleaning happens here (we did not deal with CRS transformations but only kept datasets with EPSG:4326):\n\n\nCode\n#############\n# This script will load various tree location data into one. \n# Goal: have all in one db with harmonized botanical species name, X and Y coordinate in EPSG 4326\n\n\ntree_dbs <- read_xlsx(\"2_Data/0_raw_data/opendata_trees.xlsx\") %>% \n  janitor::clean_names() %>% \n  filter(suitable == \"y\" & epsg == \"4326\") %>% \n  mutate(location = janitor::make_clean_names(location)) %>% \n  mutate(botanical_col = tolower(botanical_col)) \n  # mutate(lon_col = janitor::make_clean_names(lon_col)) %>% \n  # mutate(lat_col = janitor::make_clean_names(lat_col)) \n\n\ntree_dbs <- data.table(tree_dbs)\nfor (i in tree_dbs[,file_name]) {\n  utils::unzip(zipfile=paste(\"2_Data/0_raw_data/tree_cadastres/zip/\",i,\".zip\",sep=\"\"))\n}\n\nfor (i in 1:nrow(tree_dbs)) {\n\n  ifelse(str_detect(tree_dbs$file_name[i], \"csv\"),\n         assign(tolower(tree_dbs$location[i]),\n                janitor::clean_names(read_delim(paste0(\"2_Data/0_raw_data/tree_cadastres/\", tree_dbs$file_name[i])))),\n         assign(tolower(tree_dbs$location[i]),\n                janitor::clean_names(bind_cols(jsonlite::read_json(paste0(\"2_Data/0_raw_data/tree_cadastres/\", tree_dbs$file_name[i]), simplifyVector = TRUE)$features$properties,\n                                     jsonlite::read_json(paste0(\"2_Data/0_raw_data/tree_cadastres/\", tree_dbs$file_name[i]), simplifyVector = TRUE)$features$geometry)\n                                     )\n                )\n  )\n  print(i)\n}\n\n\n\n# # Compress all the csv files with gzip\n# tree_dbs <- data.table(tree_dbs)\n# for (i in tree_dbs[,file_name]) {\n#      db <- fread(paste(\"2_Data/0_raw_data/tree_cadastres/\",i,sep=\"\"))\n#      fwrite(db,paste(\"2_Data/0_raw_data/tree_cadastres/\",i,\".gz\",sep=\"\"),compress=\"gzip\")\n# }\n\n# tree_dbs[,file_name:=paste(file_name,\".gz\",sep=\"\")]\n# write.xlsx2(tree_dbs, \"2_Data/0_raw_data/opendata_trees.xlsx\", sheetName = \"Sheet1\", \n#             col.names = TRUE, row.names = TRUE, append = FALSE)\n\n# \n# tree_dbs <- data.table(tree_dbs)\n# for (i in tree_dbs[,location]) {\n#     print(i)\n#     assign(tolower(i), paste(fread(paste(\"2_Data/0_raw_data/tree_cadastres/\",tree_dbs[location==i,file_name],sep=\"\"))))\n# }\n              \n# too small data to automate. manually merge mutate\n#split_species <- filter(tree_dbs, str_detect(botanical_col, \"\\\\+\"))\n#mutate(tourcoing, merged_species = paste0(genre, \" \", espece)) %>% write_csv(file = \"data/opentrees/tourcoing.csv\")\n\nmerged_geo <- filter(tree_dbs, lon_col == lat_col)\nsep_geo <- filter(tree_dbs, lon_col != lat_col)\n\n\n#######\n# unmerge merged geographies\nfor (i in 1:nrow(merged_geo)) {\n  temp_db <- get(tolower(merged_geo$location[i]), envir = globalenv()) %>% \n    dplyr::select(matches(paste0(merged_geo$botanical_col[i], \"|\", merged_geo$lon_col[i]), ignore.case = TRUE)) \n  \n  temp_db <- temp_db %>% \n    separate(merged_geo$lon_col[i], into = c(\"x\", \"y\"), sep = \",\") %>% \n    mutate(x = str_remove(x, \"c\\\\(\"), y = str_remove(y, \"\\\\)\")) %>% \n    rename(species = merged_geo$botanical_col[i])\n  \n  assign(tolower(merged_geo$location[i]), temp_db)\n  print(i)\n  print(merged_geo$location[i])\n}\n\n# treat regular lat lon sources\nfor (i in 1:nrow(sep_geo)) {\n  temp_db <- get(tolower(sep_geo$location[i]), envir = globalenv()) %>% \n    dplyr::select(matches(paste0(sep_geo$botanical_col[i], \"|\", sep_geo$lon_col[i], \"|\", sep_geo$lat_col[i]), ignore.case = TRUE))\n  \n  temp_db <- temp_db %>% \n    rename(species = sep_geo$botanical_col[i], \n           x = sep_geo$lat_col[i],\n           y = sep_geo$lon_col[i]\n           )\n  \n  assign(tolower(sep_geo$location[i]), temp_db)\n  print(i)\n  print(sep_geo$location[i])\n}\n\n#bring it all together into one df\nall_trees <- tibble()\n\nfor (i in 1:nrow(tree_dbs)) {\n  temp_db <- get(tolower(tree_dbs$location[i]), envir = globalenv()) %>% \n    dplyr::select(x, y, species)\n  \n  temp_db <- temp_db %>% \n    mutate(x = as.numeric(x)) %>% \n    mutate(y = as.numeric(y)) %>% \n    mutate(db_city_origin = tree_dbs$location[i])\n  \n  assign(tolower(tree_dbs$location[i]), temp_db)\n  all_trees <- bind_rows(all_trees, temp_db)\n  print(i)\n  print(tree_dbs$location[i])\n}\n\nall_trees_centroids <- all_trees %>% \n  st_as_sf(coords = c(\"y\", \"x\"), na.fail = F) %>% \n  group_by(db_city_origin) %>% \n  summarise(st_union(geometry)) %>%\n  st_centroid() \n\n#### save it all on disk, 4 million trees\nwrite_csv(all_trees, \"2_Data/1_output/all_merged.csv\")\n\n\n#\"specie|especi|nom_lat|latboomsoort|espece|\n#nom_cientific|wetenschappelijke_naam|classe|dendro_taxon_tid|\n#  nome_scien|traeart|SPECIE|sortiment|essence_scient|gattung_lat|species|gattung|geo_point\"\n\n# all tree lists saved as csv in data folder. why csv?\n# cause majority of opendata portal tree lists were in csv so its least cumbersome conversion\n# also csv means less characters and therefore less data when things become very large. \n\n\nThis writes a file with only botanical name, X, Y and source database for the next step.\n\n\nGBIF\nThe largest amount of our tree data comes from GBIF. We simply query a bounding box of Europe for all species from our master list, enriched already with GBIF taxo IDs. You could re-generate the dataset with most recently added tree occurrences with this (GBIF takes a while to prepare the dataset and you need to have your env variables GBIF_EMAIL, GBIF_PWD, GBIF_USER configured):\n\ngbif_download <- occ_download(\n    pred_in(\"taxonKey\", tree_master_list$gbif_taxo_id),\n    #pred(\"taxonKey\", 5284884),\n    # this is the bounding box of europe\n    pred_within(\"POLYGON((-15 75,-15 30,40 30,40 75,-15 75))\"),\n    pred_lt(\"coordinateUncertaintyInMeters\",1000), #downstream processing needs 1km accuracy\n    pred(\"hasCoordinate\", TRUE),\n    pred(\"hasGeospatialIssue\", FALSE), # remove GBIF default geospatial issues\n    pred(\"occurrenceStatus\",\"PRESENT\"),\n    pred_gte(\"year\", 1960), #only keep trees seen after 1960\n    format = \"SIMPLE_CSV\")\n  #\n   occ_download_wait(gbif_download)\n\n\n\n\nGetting Bioclimatic Variables\nWe tested CHELSA, worldclim and Copernicus CDS to get bioclimatic variables. We prefer Copernicus for their extensive documentation and the convenient time frame of past bioclimatic data, from 1979-2018. This is where we expect that largest overlap with the life span of our trees from the occurrence dataset.\nOur functions getpastclimate() and getfutureclimate() can theoretically be used to switch bioclimatic data sources but we have not implemented those. These functions are used to read our Copernicus raster files.\n\n\nReading Climate Rasters into R\nIn order to extract bioclimatic variables from each species location, weâ€™ll have to load raster files into R. The function described here will solve a few complexities:\n\nWe convert raster values into units fit for anlysis.\nStacked rasters need to be treated accordingly.\nEfficiency matters greatly here, especially when it comes to RAM-efficient raster reading.\n\nWhat could and probably should not be done here: CRS-reprojections. Sometimes you may get a different CRS. Reproject outside of your ETL pipeline and then read.\n\n\nCode\n##################### Function to Get  climate Rasters ##################\n# In theory they can easily be adjusted to other climate raster providers. For some, snippets exist in the functions already but probably wont work \n# in the entire workflow of treeful. All processing here is only tested with Copernicus\n# you pass a provider and a bioclimatic var like bio01 or bio13 to the function and it returns one raster. you can stack them later on. \n\n\ngetpastclimate <- function(source = \"copernicus\", bioclim = \"bio01\") {\n  if (source == \"climateeu\") {\n    bio_path <- case_when(bioclim == \"bio01\" ~ \"MAT\",\n                          bioclim == \"bio12\" ~ \"MAP\",\n                          )\n    \n    bio_raster <- raster(paste0(\"data/climateEU/Normal_1961-1990_bioclim/\", bio_path, \".asc\"))\n\n    raster::crs(bio_raster) <- \"+proj=aea +lat_0=30 +lon_0=10 +lat_1=43 +lat_2=62 +x_0=0 +y_0=0 +ellps=intl +units=m +no_defs +type=crs\"\n    \n    bio_raster <- raster::projectRaster(bio_raster, crs = 4326)\n    \n  }\n  \n  if (source == \"copernicus\") {\n    # Get bioclimate data from copernicus. Download bioclimate file with login at https://cds.climate.copernicus.eu/\n    \n    bio_path <- toupper(bioclim)\n\n    bio_raster <- terra::rast(paste0(\"2_Data/0_raw_data/past/\", bio_path, \"_era5-to-1km_1979-2018-mean_v1.0.nc\"))\n    # convert bioclim as per copernicus documentation. for some reasone case_when does not work here.     \n    if (bioclim %in% c(\"bio01\", \"bio02\", \"bio04\", \"bio05\", \"bio06\", \"bio07\", \"bio08\", \"bio09\", \"bio10\", \"bio11\")) \n    {bio_raster <- bio_raster - 273.15\n    } else if (bioclim == \"bio12\") {bio_raster <- bio_raster*3600*24*365*1000\n    } else if (bioclim %in% c(\"bio13\", \"bio14\")) {bio_raster <- bio_raster*3600*24*30.5*1000\n    } else if (bioclim %in% c(\"bio16\", \"bio17\", \"bio18\", \"bio19\")) {bio_raster <- bio_raster*3600*24*91.3*1000\n    }\n    # a bit unclear if bio13-bio19 can and should also be comverted like bio12. probably not as theyre not on annual reference period\n\n    \n  }\n  # worldclim would be great cause it fetches all 19 bioclimatic at once. and has a great time range from 1970-2000. \n  # but units of each var are stange. \n\n  if (source == \"worldclim\") {\n    \n    bio_path <- case_when(bioclim == \"bio01\" ~ \"bio_1\",\n                          bioclim == \"bio02\" ~ \"bio_2\",\n                          bioclim == \"bio03\" ~ \"bio_3\",\n                          bioclim == \"bio04\" ~ \"bio_4\",\n                          bioclim == \"bio05\" ~ \"bio_5\",\n                          bioclim == \"bio06\" ~ \"bio_6\",\n                          bioclim == \"bio07\" ~ \"bio_7\",\n                          bioclim == \"bio08\" ~ \"bio_8\",\n                          bioclim == \"bio09\" ~ \"bio_9\",\n                          bioclim == \"bio10\" ~ \"bio_10\",\n                          bioclim == \"bio11\" ~ \"bio_11\",\n                          bioclim == \"bio12\" ~ \"bio_12\",\n                          bioclim == \"bio13\" ~ \"bio_13\",\n                          bioclim == \"bio14\" ~ \"bio_14\",\n                          bioclim == \"bio15\" ~ \"bio_15\",\n                          bioclim == \"bio16\" ~ \"bio_16\",\n                          bioclim == \"bio17\" ~ \"bio_17\",\n                          bioclim == \"bio18\" ~ \"bio_18\",\n                          bioclim == \"bio19\" ~ \"bio_19\"\n    )\n    \n    bio_raster <- raster(paste0(\"2_Data/1_output/worldclim_cropped/wc2.1_30s_\", bio_path, \".tif\"))\n    \n\n  }\n  \n  if (source == \"chelsa\") {\n    \n    bio_path <- case_when(bioclim == \"bio01\" ~ \"bio1\",\n                          bioclim == \"bio12\" ~ \"bio12\"\n    )\n    \n    bio_raster <- raster(paste0(\"2_Data/1_output/CHELSA_cropped/CHELSA_\", bio_path, \"_1981-2010_V.2.1.tif\"))\n    \n    if (bioclim == \"bio01\") {bio_raster <- raster::calc(bio_raster, function(x) { x / 10 - 273.15 })\n    } else if (bioclim == \"bio12\") {\n      bio_raster <- raster::calc(bio_raster, function(x) { x / 10})\n    }\n    \n    \n  }\n  \n  \n  return(bio_raster)\n  rm(bio_raster)\n  \n}\n\n\n#################### Get Future Climate ##################\n# for now using climate projection model MPI-ESM1-2-LR and socio-econ pathway 245 \n\ngetfutureclimate <- function(source = \"copernicus\", bioclim = \"bio01\") {\n  if(source == \"chelsa\") {\n    future_raster <- raster::stack(c(\"2_Data/1_output/CHELSA_cropped/CHELSA_bio1_2041-2070_gfdl-esm4_ssp370_V.2.1.tif\", \n                    \"2_Data/1_output/CHELSA_cropped/CHELSA_bio12_2041-2070_gfdl-esm4_ssp370_V.2.1.tif\"))\n    names(future_raster)[1] <- \"bio01\"\n    names(future_raster)[2] <- \"bio12\"\n    future_raster$bio01 <- raster::calc(future_raster$bio01, function(x) { x / 10 - 273.15 })\n    future_raster$bio12 <- raster::calc(future_raster$bio12, function(x) { x / 10})\n    \n  } else if (source == \"worldclim\") {\n    future_raster <- geodata::cmip6_tile(model = \"MPI-ESM1-2-LR\", lon = 11.01684, lat = 51.28691, \n                                         ssp = \"245\", time = \"2041-2060\", var = \"bioc\", path = \"2_Data/0_raw_data/\", res = 5)\n    names(future_raster)[1] <- \"bio01\"\n    names(future_raster)[12] <- \"bio12\"\n  } else if (source == \"copernicus\") {\n\n    bio_path <- toupper(bioclim)\n\n    bio_raster <- terra::rast(paste0(\"2_Data/0_raw_data/future/\", bio_path, \"_hadgem2-cc_rcp45_r1i1p1_1960-2099-mean_v1.0.nc\"))\n    #$X2050.01.01\n    names(bio_raster) <- terra::time(bio_raster)\n    bio_raster <- bio_raster$`2050-01-01`\n    # convert bioclim as per copernicus documentation. for some reason case_when does not work here.     \n    if (bioclim %in% c(\"bio01\", \"bio02\", \"bio04\", \"bio05\", \"bio06\", \"bio07\", \"bio08\", \"bio09\", \"bio10\", \"bio11\")) \n    {bio_raster <- bio_raster - 273.15\n    } else if (bioclim == \"bio12\") {bio_raster <- bio_raster*3600*24*365*1000\n    } else if (bioclim %in% c(\"bio13\", \"bio14\")) {bio_raster <- bio_raster*3600*24*30.5*1000\n    } else if (bioclim %in% c(\"bio16\", \"bio17\", \"bio18\", \"bio19\")) {bio_raster <- bio_raster*3600*24*91.3*1000\n    }\n    \n  }\n  \n  return(bio_raster)\n  rm(bio_raster)\n}\n\ngetsoilproperties <- function(variable = \"STU_EU_DEPTH_ROOTS\") {\n  soil_layer <- terra::rast(paste0(\"2_Data/0_raw_data/soil/\", variable, \"_4326.tif\"))\n  return(soil_layer)\n  rm(soil_layer)\n}\n\n\n\n\nMerging Tree Locations and Bioclimatic Variables\nOn efficient functions and lots of RAM\n\n\nWriting to PostGIS DB\nThe bioclimatic and soil rasters used in this project are several GBs large and usually do not fit into memory. This snipped reads them as raster stack, writes the stack to the PostGIS DB and removes it.\n\n\n\n\n\n\nWarning\n\n\n\nThis section relies on the package rpostgis. Currently, thereâ€™s no simple other way to write raster data to postGIS from R, neither sf nor terra nor stars. See this discussion\n\n\n\nprint(\"large data transfer out starting. Writing all Rasters as rasterstack to Postgres\")\n\ncon <- DBI::dbConnect(RPostgres::Postgres(), \n                      dbname = Sys.getenv(\"POSTGRES_DB\"),\n                      host= \"192.168.178.148\", \n                      port=\"5432\",\n                      user=\"postgres\",\n                      password=Sys.getenv(\"POSTGRES_PW\"))\n\n# \n# pastbio01 <- getpastclimate(source = \"copernicus\", bioclim = \"bio01\")\n# \n# \n# pastbio12 <- getpastclimate(source = \"copernicus\", bioclim = \"bio12\")\n# crs(pastbio12) <- \"+proj=longlat +datum=WGS84 +no_defs +type=crs\"\n# # we set this proj here and it seems to stick to R raster object. \n# # when writing into postgis, there SRID appears to be 3395\n\nif (!RPostgres::dbExistsTable(conn = con, name = \"past\")) {\n  cat(\"Raster layers do not exist in Postgres.\")\n  \n  print(\"Reading in Copernicus past raster\")\n  past <- raster::stack(raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio01\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio02\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio03\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio04\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio05\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio06\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio07\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio08\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio09\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio10\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio11\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio12\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio13\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio14\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio15\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio16\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio17\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio18\")),\n                        raster::raster(getpastclimate(source = \"copernicus\", bioclim = \"bio19\"))\n  )\n  print(\"Writing Copernicus Past to DB\")\n  rpostgis::pgWriteRast(con,\n                        name = \"past\", raster = past, overwrite = TRUE\n  )\n  rm(past)\n  gc()\n  \n  print(\"starting with reading Copernicus Future\")\n  future <- raster::stack(raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio01\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio02\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio03\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio04\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio05\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio06\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio07\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio08\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio09\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio10\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio11\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio12\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio13\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio14\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio15\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio16\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio17\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio18\")),\n                          raster::raster(getfutureclimate(source = \"copernicus\", bioclim = \"bio19\"))\n  )\n  \n  print(\"Writing Copernicus Future to DB\")\n  rpostgis::pgWriteRast(con,\n                        name = \"future\", raster = future, overwrite = TRUE\n  )\n  rm(future)\n  gc()\n  \n  print(\"Reading in Soil Rasters\")\n  soil <- raster::stack(raster::raster(getsoilproperties(\"STU_EU_DEPTH_ROOTS\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_CLAY\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_CLAY\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_SAND\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_SAND\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_SILT\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_SILT\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_OC\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_OC\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_BD\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_BD\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_GRAVEL\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_GRAVEL\")),\n                        raster::raster(getsoilproperties(\"SMU_EU_T_TAWC\")),\n                        raster::raster(getsoilproperties(\"SMU_EU_S_TAWC\")),\n                        raster::raster(getsoilproperties(\"STU_EU_T_TAWC\")),\n                        raster::raster(getsoilproperties(\"STU_EU_S_TAWC\"))\n  )\n  \n  \n  print(\"Writing Soil to DB\")\n  rpostgis::pgWriteRast(con,\n                        name = \"soil\", raster = soil, overwrite = TRUE\n  )\n  rm(soil)\n  gc()\n  \n} else {\n  cat(\"Raster layers exist in Postgres. Skipping. \")\n}\n\n\n\nDBI::dbDisconnect(conn = con)"
  },
  {
    "objectID": "how-it-works.html#quarto",
    "href": "how-it-works.html#quarto",
    "title": "1Â  How it works",
    "section": "1.1 Quarto",
    "text": "1.1 Quarto"
  },
  {
    "objectID": "how-it-works.html#running-code",
    "href": "how-it-works.html#running-code",
    "title": "1Â  How it works",
    "section": "1.2 Running Code",
    "text": "1.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n conn <- DBI::dbConnect(RPostgres::Postgres(),\n                        dbname = \"treeful-test\",\n                        host= Sys.getenv(\"POSTGRES_HOST\"),\n                        port=\"5432\",\n                        user=\"postgres\",\n                        password=Sys.getenv(\"POSTGRES_PW\"))\n\ntree_db <- data.table::fread(\"../../../data/tree_db.csv\")\n\ntree_occurrence <- DBI::dbGetQuery(conn, paste0(\n      \"SELECT * FROM tree_dbs;\"))\n\ntree_occurrence <- sf::st_read(conn, layer = \"tree_dbs\")\n\n\noptions(scipen=100000000)\nlibrary(librarian)\nshelf(tidyverse, sf, hrbrthemes)\n\n\ntree_occurrence %>% \n  #slice(1:1000000) %>% \n  st_drop_geometry() %>% \n  group_by(db) %>% \n  summarise(n=n()) %>% \n  ggplot(aes(x = reorder(db, +n), y = n)) +\n    geom_col(fill = \"#6e944eff\") +\n    theme_ipsum() +\n    coord_flip() +\n    labs(title = \"Tree Occurrences imported\", x = \"Data Source\", y = \"n\")\n\n\noptions(scipen=100000000)\neu_grid <- sf::st_make_grid(sf::st_bbox(tree_occurrence),\n  n = c(100,100),\n  what = 'polygons',\n  square = FALSE,\n  flat_topped = TRUE) %>%\n  sf::st_as_sf() %>% \n  tibble::rownames_to_column(var = \"grid_id\")\n\ntree_occurrence <- sf::st_join(st_make_valid(tree_occurrence), eu_grid, join = st_within)\ntree_grid <- tree_occurrence %>%  \n  sf::st_drop_geometry() %>% \n  dplyr::select(master_list_name, db, grid_id)\ntree_count <- tree_grid %>% \n  dplyr::group_by(grid_id) %>% \n  dplyr::count()\n\neu_grid <- eu_grid %>% \n  dplyr::left_join(tree_count)\n\n#tmap::tm_shape(eu_grid) + tmap::tm_polygons(col = \"n\", alpha = 0.5, id = \"n\", style = \"log10_pretty\")\n  \nneutralocre <- \"#f5efe2ff\"\n\n  ggplot2::ggplot() +\n  ggplot2::geom_sf(data = eu_grid, aes(fill = n), color = \"white\", lwd = 0) +\n  viridis::scale_fill_viridis(direction = -1, option = \"D\", na.value = NA, trans = \"log\", breaks = c(10,100,1000,10000,100000)) +\n  ggplot2::labs(title = paste0(nrow(tree_occurrence), \" Tree Locations\"), fill = \"\") +\n  ggplot2::theme_void() +\n  ggplot2::theme(legend.position = \"bottom\", \n        legend.direction = \"horizontal\", legend.key.width = ggplot2::unit(2, \"cm\"))\n\n\nlibrary(raster)\n\nsource(\"../1_ETL/3_R/3_fn_get_climate_rasters.R\")\ngetpastclimate(source = \"copernicus\", bioclim = \"bio01\")\nbio_path <- \"BIO01\"\nbio01 <- raster(paste0(\"../1_ETL/2_Data/0_raw_data/copernicus/\", bio_path, \"_era5-to-1km_1979-2018-mean_v1.0.nc\"))\nbio01 <- calc(bio01, function(x) {x - 273.15})\nharz <- osmdata::opq_osm_id(id = 3734731, type = \"relation\") %>%\n  osmdata::osmdata_sf()\nharz <- st_make_valid(harz$osm_multipolygons)\n\nharz_bio <- raster::crop(bio01, extent(harz))\nharz_bio <- as.data.frame(harz_bio, xy = TRUE) \n\nggplot() +\n  geom_raster(data = harz_bio, aes(x = x, y = y, fill = layer)) + \n  scale_fill_viridis() +\n  theme_light() +\n  coord_quickmap() +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\", axis.ticks = element_blank(), axis.text = element_blank(), axis.title = element_blank()) +\n  labs(fill = \"Durchschnittstemperatur 1979 - 2018\", title = \"Copernicus Temperatur am Harz 1979 - 2018\")\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "index.html#shiny-app-development",
    "href": "index.html#shiny-app-development",
    "title": "Treeful Docs",
    "section": "Shiny App Development",
    "text": "Shiny App Development\nThe shiny app itself is developed as package using the golem framework."
  },
  {
    "objectID": "index.html#database-deployment",
    "href": "index.html#database-deployment",
    "title": "Treeful Docs",
    "section": "Database Deployment",
    "text": "Database Deployment\n\nfrom postgis/postgis\n# need to enable raster extentions\n\n\nCOPY ./initialize-raster.sh /docker-entrypoint-initdb.d/initialize-raster.sh\n# CREATE EXTENSION postgis_raster;\n# SET postgis.gdal_enabled_drivers = 'ENABLE_ALL';\n# SELECT name, default_version,installed_version\n# FROM pg_available_extensions WHERE name LIKE 'postgis%' or name LIKE 'address%';"
  },
  {
    "objectID": "index.html#shiny-app-deployment",
    "href": "index.html#shiny-app-deployment",
    "title": "Treeful Docs",
    "section": "Shiny App Deployment",
    "text": "Shiny App Deployment\nThanks to our Postgis database, the Shiny app itself is rather lightweight, does not need large files to be transferred and only does a minimum of plotting. The docker-compose.yml file specifies that both the backend postgres database and the frontend shiny application will be launched. You could also run them on separate hosts.\n\nGenerate Dockerfile from app. Golem handles this. It is wise to double-check the Dockerfile before buidling. When adding new packages, ensure theyâ€™re at the end of the Dockerfile in order to leverage build caching.\nOn your host: clone the repository with git clone https://github.com/3ful/treeful.git and move to the directory with cd ./treeful\nRun stack with swarm and compose\n\nlaunch your docker swarm: docker swarm init\nIn order to serve the same image to all nodes you need a docker registry. Create one with: docker service create --name registry --publish published=5000,target=5000 registry:2\nPush images to registry: docker compose push\nNow, deploy your swarm: docker stack deploy --compose-file docker-compose.yml treeful\nNow you can easily scale up your shiny app to 3 replicas with docker service scale treeful_frontend=3\nYou can watch visitors of your frontend app with docker service logs treeful_frontend -f"
  }
]